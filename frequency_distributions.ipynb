{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distributions\n",
    "## Simplifying Data\n",
    "![Steps](https://s3.amazonaws.com/dq-content/285/s1m3_workflow.svg)\n",
    "\n",
    "Our capacity to understand a data set just by looking at it in a table format is limited, and it decreases dramatically as the size of the data set increases. To be able to analyze data, we need to find ways to simplify it.\n",
    "\n",
    "One way to simplify this data set is to select a variable, count how many times each unique value occurs, and represent the `frequencies` (the number of times a unique value occurs) in a table.\n",
    "\n",
    "Try to get a sense for how difficult it is to analyze the basketball data set in its original form.\n",
    "* Read in the basketball data set (the name of the CSV file is wnba.csv) using pd.read_csv().\n",
    "* Using DataFrame.shape, find the number of rows and columns of the data set.\n",
    "* Print the entire data set, and try to analyze the output to find some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (143, 32)\n",
      "Dataset:                          Name Team  Pos  Height  Weight        BMI  \\\n",
      "0               Aerial Powers  DAL    F     183    71.0  21.200991   \n",
      "1                 Alana Beard   LA  G/F     185    73.0  21.329438   \n",
      "2                Alex Bentley  CON    G     170    69.0  23.875433   \n",
      "3             Alex Montgomery  SAN  G/F     185    84.0  24.543462   \n",
      "4                Alexis Jones  MIN    G     175    78.0  25.469388   \n",
      "5             Alexis Peterson  SEA    G     170    63.0  21.799308   \n",
      "6               Alexis Prince  PHO    G     188    81.0  22.917610   \n",
      "7               Allie Quigley  CHI    G     178    64.0  20.199470   \n",
      "8                Allisha Gray  DAL    G     185    76.0  22.205990   \n",
      "9           Allison Hightower  WAS    G     178    77.0  24.302487   \n",
      "10               Alysha Clark  SEA    F     180    76.0  23.456790   \n",
      "11              Alyssa Thomas  CON    F     188    84.0  23.766410   \n",
      "12            Amanda Zahui B.   NY    C     196   113.0  29.414827   \n",
      "13               Amber Harris  CHI    F     196    88.0  22.907122   \n",
      "14               Aneika Henry  ATL  F/C     193    87.0  23.356332   \n",
      "15             Angel Robinson  PHO  F/C     198    88.0  22.446689   \n",
      "16                Asia Taylor  WAS    F     185    76.0  22.205990   \n",
      "17            Bashaara Graves  CHI    F     188    91.0  25.746944   \n",
      "18              Breanna Lewis  DAL    C     196    93.0  24.208663   \n",
      "19            Breanna Stewart  SEA  F/C     193    77.0  20.671696   \n",
      "20               Bria Hartley   NY    G     173    66.0  22.052190   \n",
      "21                Bria Holmes  ATL    G     185    77.0  22.498174   \n",
      "22             Briann January  IND    G     173    65.0  21.718066   \n",
      "23              Brionna Jones  CON    F     191   104.0  28.507990   \n",
      "24              Brittany Boyd   NY    G     175    71.0  23.183673   \n",
      "25            Brittney Griner  PHO    C     206    93.0  21.915355   \n",
      "26             Brittney Sykes  ATL    G     175    66.0  21.551020   \n",
      "27             Camille Little  PHO    F     188    82.0  23.200543   \n",
      "28             Candace Parker   LA  F/C     193    79.0  21.208623   \n",
      "29             Candice Dupree  IND    F     188    81.0  22.917610   \n",
      "30           Cappie Pondexter  CHI    G     175    73.0  23.836735   \n",
      "31             Carolyn Swords  SEA    C     198    95.0  24.232221   \n",
      "32               Cayla George  PHO    C     193    87.0  23.356332   \n",
      "33               Chelsea Gray   LA    G     180    77.0  23.765432   \n",
      "34            Cheyenne Parker  CHI    F     193    86.0  23.087868   \n",
      "35        Clarissa dos Santos  SAN    C     185    89.0  26.004383   \n",
      "36             Courtney Paris  DAL    C     193   113.0  30.336385   \n",
      "37       Courtney Vandersloot  CHI    G     173    66.0  22.052190   \n",
      "38          Courtney Williams  CON    G     173    62.0  20.715694   \n",
      "39          Crystal Langhorne  SEA  F/C     188    84.0  23.766410   \n",
      "40             Damiris Dantas  ATL    C     191    89.0  24.396261   \n",
      "41             Danielle Adams  CON  F/C     185   108.0  31.555880   \n",
      "42          Danielle Robinson  PHO    G     175    57.0  18.612245   \n",
      "43              Dearica Hamby  SAN    F     191    86.0  23.573915   \n",
      "44           Devereaux Peters  IND    F     188    79.0  22.351743   \n",
      "45              Diana Taurasi  PHO    G     183    74.0  22.096808   \n",
      "46          Elena Delle Donne  WAS  G/F     196    85.0  22.126197   \n",
      "47         Elizabeth Williams  ATL  F/C     191    87.0  23.848030   \n",
      "48                Emma Cannon  PHO    F     188    86.0  24.332277   \n",
      "49             Emma Meesseman  WAS    C     193    83.0  22.282477   \n",
      "50           Epiphanny Prince   NY    G     175    81.0  26.448980   \n",
      "51              Erica Wheeler  IND    G     170    65.0  22.491349   \n",
      "52             Ã‰rika de Souza  SAN    C     196    86.0  22.386506   \n",
      "53             Erlana Larkins  IND    F     185    93.0  27.173119   \n",
      "54             Essence Carson   LA  G/F     183    74.0  22.096808   \n",
      "55             Evelyn Akhator  DAL    F     191    82.0  22.477454   \n",
      "56              Glory Johnson  DAL    F     191    77.0  21.106878   \n",
      "57              Imani Boyette  ATL    C     201    88.0  21.781639   \n",
      "58          Isabelle Harrison  SAN    C     191    83.0  22.751569   \n",
      "59                Ivory Latta  WAS    G     168    63.0  22.321429   \n",
      "60            Jantel Lavender   LA    C     193    84.0  22.550941   \n",
      "61             Jasmine Thomas  CON    G     175    66.0  21.551020   \n",
      "62            Jazmon Gwathmey  IND    G     188    65.0  18.390675   \n",
      "63            Jeanette Pohlen  IND    G     183    78.0  23.291230   \n",
      "64            Jennifer Hamson  IND    C     201    95.0  23.514269   \n",
      "65            Jessica Breland  CHI    F     191    77.0  21.106878   \n",
      "66                Jewell Loyd  SEA    G     178    67.0  21.146320   \n",
      "67                Jia Perkins  MIN    G     173    75.0  25.059307   \n",
      "68              Jonquel Jones  CON  F/C     198    86.0  21.936537   \n",
      "69              Jordan Hooper  CHI    F     188    84.0  23.766410   \n",
      "70                Kaela Davis  DAL    G     188    77.0  21.785876   \n",
      "71             Kahleah Copper  CHI  G/F     185    70.0  20.452885   \n",
      "72     Kaleena Mosqueda-Lewis  SEA    F     180    82.0  25.308642   \n",
      "73     Karima Christmas-Kelly  DAL  G/F     183    82.0  24.485652   \n",
      "74            Kayla Alexander  SAN    C     193    88.0  23.624795   \n",
      "75              Kayla McBride  SAN  G/F     180    79.0  24.382716   \n",
      "76             Kayla Pedersen  CON    F     193    86.0  23.087868   \n",
      "77             Kayla Thornton  DAL    F     185    86.0  25.127831   \n",
      "78             Keisha Hampton  CHI    F     185    78.0  22.790358   \n",
      "79                Kelsey Plum  SAN    G     173    66.0  22.052190   \n",
      "80                 Kia Vaughn   NY    C     193    90.0  24.161722   \n",
      "81                Kiah Stokes   NY    C     191    87.0  23.848030   \n",
      "82             Kristi Toliver  WAS    G     170    59.0  20.415225   \n",
      "83             Krystal Thomas  WAS    C     196    88.0  22.907122   \n",
      "84           Lanay Montgomery  SEA    C     196    96.0  24.989588   \n",
      "85          Layshia Clarendon  ATL    G     175    64.0  20.897959   \n",
      "86           Leilani Mitchell  PHO    G     165    58.0  21.303949   \n",
      "87              Lindsay Allen   NY    G     173    65.0  21.718066   \n",
      "88             Lindsay Whalen  MIN    G     175    78.0  25.469388   \n",
      "89              Lynetta Kizer  CON    C     193   104.0  27.920213   \n",
      "90            Maimouna Diarra   LA    C     198    90.0  22.956841   \n",
      "91               Makayla Epps  CHI    G     178     NaN        NaN   \n",
      "92            Marissa Coleman  IND  G/F     185    73.0  21.329438   \n",
      "93               Matee Ajavon  ATL    G     173    73.0  24.391059   \n",
      "94                 Maya Moore  MIN    F     183    80.0  23.888441   \n",
      "95             Monique Currie  PHO  G/F     183    80.0  23.888441   \n",
      "96                Morgan Tuck  CON    F     188    91.0  25.746944   \n",
      "97           Moriah Jefferson  SAN    G     168    55.0  19.486961   \n",
      "98            Natalie Achonwa  IND    C     193    83.0  22.282477   \n",
      "99              Natasha Cloud  WAS    G     183    73.0  21.798202   \n",
      "100            Natasha Howard  MIN    F     188    75.0  21.220009   \n",
      "101      Nayo Raincock-Ekunwe   NY  F/C     188    79.0  22.351743   \n",
      "102                Nia Coffey  SAN    F     185    77.0  22.498174   \n",
      "103            Nneka Ogwumike   LA    F     188    79.0  22.351743   \n",
      "104              Noelle Quinn  SEA    G     183    81.0  24.187046   \n",
      "105              Odyssey Sims   LA    G     173    73.0  24.391059   \n",
      "106          Plenette Pierson  MIN  F/C     188    88.0  24.898144   \n",
      "107             Rachel Banham  CON    G     175    76.0  24.816327   \n",
      "108            Ramu Tokashiki  SEA    F     193    80.0  21.477087   \n",
      "109             Rebecca Allen   NY  G/F     188    74.0  20.937076   \n",
      "110          Rebekkah Brunson  MIN    F     188    84.0  23.766410   \n",
      "111          Renee Montgomery  MIN    G     170    63.0  21.799308   \n",
      "112           Riquna Williams   LA    G     170    75.0  25.951557   \n",
      "113             Sami Whitcomb  SEA    G     178    66.0  20.830703   \n",
      "114             Sancho Lyttle  ATL    F     193    79.0  21.208623   \n",
      "115            Sandrine Gruda   LA  F/C     193    84.0  22.550941   \n",
      "116              Saniya Chong  DAL    G     173    64.0  21.383942   \n",
      "117          Seimone Augustus  MIN  G/F     183    77.0  22.992624   \n",
      "118            Sequoia Holmes  SAN    G     185    70.0  20.452885   \n",
      "119  Shatori Walker-Kimbrough  WAS    G     180    64.0  19.753086   \n",
      "120          Shavonte Zellous   NY    G     178    85.0  26.827421   \n",
      "121               Shay Murphy  SAN    G     180    74.0  22.839506   \n",
      "122        Shekinna Stricklen  CON  G/F     188    81.0  22.917610   \n",
      "123           Shenise Johnson  IND    G     180    78.0  24.074074   \n",
      "124      Skylar Diggins-Smith  DAL    G     175    66.0  21.551020   \n",
      "125           Stefanie Dolson  CHI    C     196    97.0  25.249896   \n",
      "126          Stephanie Talbot  PHO    G     185    87.0  25.420015   \n",
      "127                  Sue Bird  SEA    G     175    68.0  22.204082   \n",
      "128             Sugar Rodgers   NY    G     175    75.0  24.489796   \n",
      "129             Sydney Colson  SAN    G     173    64.0  21.383942   \n",
      "130              Sydney Wiese   LA    G     183    68.0  20.305175   \n",
      "131             Sylvia Fowles  MIN    C     198    96.0  24.487297   \n",
      "132              Tamera Young  ATL  G/F     188    77.0  21.785876   \n",
      "133               Tayler Hill  WAS    G     175    66.0  21.551020   \n",
      "134             Temi Fagbenle  MIN    C     193    89.0  23.893259   \n",
      "135         Theresa Plaisance  DAL    F     196    91.0  23.688047   \n",
      "136            Tianna Hawkins  WAS    F     191    87.0  23.848030   \n",
      "137       Tierra Ruffin-Pratt  WAS    G     178    83.0  26.196187   \n",
      "138             Tiffany Hayes  ATL    G     178    70.0  22.093170   \n",
      "139           Tiffany Jackson   LA    F     191    84.0  23.025685   \n",
      "140          Tiffany Mitchell  IND    G     175    69.0  22.530612   \n",
      "141              Tina Charles   NY  F/C     193    84.0  22.550941   \n",
      "142             Yvonne Turner  PHO    G     175    59.0  19.265306   \n",
      "\n",
      "    Birth_Place           Birthdate  Age              College Experience  \\\n",
      "0            US    January 17, 1994   23       Michigan State          2   \n",
      "1            US        May 14, 1982   35                 Duke         12   \n",
      "2            US    October 27, 1990   26           Penn State          4   \n",
      "3            US   December 11, 1988   28         Georgia Tech          6   \n",
      "4            US      August 5, 1994   23               Baylor          R   \n",
      "5            US       June 20, 1995   22             Syracuse          R   \n",
      "6            US    February 5, 1994   23               Baylor          R   \n",
      "7            US       June 20, 1986   31               DePaul          8   \n",
      "8            US    October 20, 1992   24       South Carolina          2   \n",
      "9            US        June 4, 1988   29                  LSU          5   \n",
      "10           US        July 7, 1987   30     Middle Tennessee          6   \n",
      "11           US    December 4, 1992   24             Maryland          3   \n",
      "12           SE      August 9, 1993   24            Minnesota          3   \n",
      "13           US    January 16, 1988   29               Xavier          3   \n",
      "14           JM   February 13, 1986   31              Florida          6   \n",
      "15           US     August 30, 1995   21        Arizona State          1   \n",
      "16           US     August 22, 1991   26           Louisville          3   \n",
      "17           US      March 17, 1994   23            Tennessee          1   \n",
      "18           US       June 22, 1994   23         Kansas State          R   \n",
      "19           US     August 27, 1994   22          Connecticut          2   \n",
      "20           US  September 30, 1992   24          Connecticut          4   \n",
      "21           US      April 19, 1994   23        West Virginia          R   \n",
      "22           US    November 1, 1987   29        Arizona State          9   \n",
      "23           US   December 18, 1995   21             Maryland          R   \n",
      "24           US    November 6, 1993   23          UC Berkeley          3   \n",
      "25           US    October 18, 1990   26               Baylor          5   \n",
      "26           US        July 2, 1994   23              Rutgers         10   \n",
      "27           US    January 18, 1985   32       North Carolina         11   \n",
      "28           US      April 19, 1986   31            Tennessee         10   \n",
      "29           US   February 25, 1984   33               Temple         12   \n",
      "30           US        July 1, 1983   34              Rutgers         11   \n",
      "31           US       July 19, 1989   28       Boston College          6   \n",
      "32           AU      April 20, 1987   30              Georgia          1   \n",
      "33           US     August 10, 1992   25                 Duke          3   \n",
      "34           US     August 22, 1992   25     Middle Tennessee          2   \n",
      "35           BR     October 3, 1988   28               Brazil          4   \n",
      "36           US  September 21, 1987   29             Oklahoma          7   \n",
      "37           US      August 2, 1989   28              Gonzaga          6   \n",
      "38           US    November 5, 1994   22        South Florida          1   \n",
      "39           US    October 27, 1986   30             Maryland         10   \n",
      "40           BR   November 17, 1992   24               Brazil          4   \n",
      "41           US   February 19, 1989   28            Texas A&M          5   \n",
      "42           US     October 5, 1989   27             Oklahoma          7   \n",
      "43           US       June 11, 1993   24          Wake Forest          2   \n",
      "44           US     August 10, 1989   28           Notre Dame          6   \n",
      "45           US    November 6, 1982   34          Connecticut         13   \n",
      "46           US         May 9, 1989   28             Delaware          5   \n",
      "47           US       June 23, 1993   24                 Duke          3   \n",
      "48           US     January 6, 1989   28      Central Florida          R   \n",
      "49           BE        May 13, 1993   24              Belgium          5   \n",
      "50           US    November 1, 1988   28              Rutgers          8   \n",
      "51           US    February 5, 1991   26              Rutgers          3   \n",
      "52           BR   September 3, 1982   34               Brazil         13   \n",
      "53           US    February 4, 1986   31       North Carolina          9   \n",
      "54           US       July 28, 1986   31              Rutgers         10   \n",
      "55           NG       March 2, 1995   22             Kentucky          R   \n",
      "56           US       July 27, 1990   27            Tennessee          4   \n",
      "57           US   November 10, 1992   24                Texas          1   \n",
      "58           US  September 27, 1993   23             Kentucky          3   \n",
      "59           US  September 25, 1984   32       North Carolina         12   \n",
      "60           US   December 11, 1988   28           Ohio State          7   \n",
      "61           US  September 30, 1989   27                 Duke          6   \n",
      "62           PR    January 24, 1993   24        James Madison          2   \n",
      "63           US    February 5, 1989   28             Stanford          6   \n",
      "64           US    January 23, 1992   25        Brigham Young          1   \n",
      "65           US   February 23, 1988   29       North Carolina          5   \n",
      "66           US        May 10, 1993   24           Notre Dame          3   \n",
      "67           US   February 23, 1982   35           Texas Tech         14   \n",
      "68           BS         May 1, 1994   23    George Washington          1   \n",
      "69           US   February 20, 1992   25             Nebraska          3   \n",
      "70           US      March 15, 1995   22       South Carolina          R   \n",
      "71           US     August 28, 1994   22              Rutgers          1   \n",
      "72           US      March 11, 1993   24          Connecticut          3   \n",
      "73           US    November 9, 1989   27                 Duke          6   \n",
      "74           CA         May 1, 1991   26        Arizona State          4   \n",
      "75           US       June 25, 1992   25           Notre Dame          3   \n",
      "76           US      April 14, 1989   28             Stanford          5   \n",
      "77           US    October 20, 1992   24        Texasâ€“El Paso          2   \n",
      "78           US   February 22, 1990   27               DePaul          1   \n",
      "79           US     August 24, 1994   23           Washington          R   \n",
      "80           US    January 24, 1987   30              Rutgers          9   \n",
      "81           US      March 30, 1993   24          Connecticut          3   \n",
      "82           US    January 27, 1987   30             Maryland          9   \n",
      "83           US     October 6, 1989   27                 Duke          6   \n",
      "84           US  September 17, 1993   23        West Virginia          R   \n",
      "85           US    February 5, 1991   26          UC Berkeley          5   \n",
      "86           US       June 15, 1985   32                 Utah          9   \n",
      "87           US      March 20, 1995   22           Notre Dame          R   \n",
      "88           US   September 5, 1982   34            Minnesota         14   \n",
      "89           US       April 4, 1990   27             Maryland          5   \n",
      "90           SN    January 30, 1991   26               Sengal          R   \n",
      "91           US        June 6, 1995   22             Kentucky          R   \n",
      "92           US       April 1, 1987   30             Maryland          9   \n",
      "93           US        July 5, 1986   31            Syracruse          R   \n",
      "94           US    November 6, 1989   27          Connecticut          7   \n",
      "95           US   February 25, 1983   34                 Duke         11   \n",
      "96           US      April 30, 1994   23          Connecticut          1   \n",
      "97           US      August 3, 1994   23          Connecticut          1   \n",
      "98           CA   November 22, 1992   24           Notre Dame          3   \n",
      "99           US   February 22, 1992   25       Saint Joseph's          3   \n",
      "100          US    February 9, 1991   26        Florida State          4   \n",
      "101          CA     August 29, 1991   25         Simon Fraser          R   \n",
      "102          US        May 21, 1995   22         Northwestern          R   \n",
      "103          US    February 7, 1990   27             Stanford          6   \n",
      "104          US       March 1, 1985   32                 UCLA         11   \n",
      "105          US       July 13, 1992   25               Baylor          4   \n",
      "106          US     August 31, 1981   35           Texas Tech         15   \n",
      "107          US       July 15, 1993   24            Minnesota          2   \n",
      "108          JP    November 6, 1991   25                Japan          1   \n",
      "109          AU       June 11, 1992   25            Australia          3   \n",
      "110          US   November 12, 1981   35           Georgetown         14   \n",
      "111          US   February 12, 1986   31          Connecticut          9   \n",
      "112          US        May 28, 1990   27           Miami (FL)          5   \n",
      "113          US       July 20, 1988   29           Washington          R   \n",
      "114          ES  September 20, 1983   33              Houston         13   \n",
      "115          FR       June 25, 1987   30               France          5   \n",
      "116          US       June 27, 1994   23          Connecticut          R   \n",
      "117          US      April 30, 1984   33                  LSU         12   \n",
      "118          US       June 13, 1986   31                 UNLV          2   \n",
      "119          US        May 18, 1995   22             Maryland          R   \n",
      "120          US     August 28, 1986   30           Pittsburgh          9   \n",
      "121          US      April 15, 1985   32  Southern California          9   \n",
      "122          US       July 30, 1990   27            Tennessee          5   \n",
      "123          US  September 12, 1990   26           Miami (FL)          6   \n",
      "124          US    February 8, 1990   27           Notre Dame          4   \n",
      "125          US      August 1, 1992   25          Connecticut          3   \n",
      "126          AU   December 20, 1990   26            Australia          R   \n",
      "127          US    October 16, 1980   36          Connecticut         15   \n",
      "128          US     August 12, 1989   28           Georgetown          6   \n",
      "129          US        June 8, 1989   28            Texas A&M          3   \n",
      "130          US       July 13, 1992   25         Oregon State          R   \n",
      "131          US       June 10, 1985   32                  LSU         10   \n",
      "132          US    October 30, 1986   30            Tennessee          9   \n",
      "133          US    October 23, 1990   26           Ohio State          5   \n",
      "134          UK      August 9, 1992   25  Southern California          R   \n",
      "135          US        May 18, 1992   25                  LSU          4   \n",
      "136          US    February 3, 1991   26             Maryland          4   \n",
      "137          US    November 4, 1991   25       North Carolina          5   \n",
      "138          US  September 20, 1989   27          Connecticut          6   \n",
      "139          US      April 26, 1985   32                Texas          9   \n",
      "140          US  September 23, 1984   32       South Carolina          2   \n",
      "141          US        May 12, 1988   29          Connecticut          8   \n",
      "142          US    October 13, 1987   29             Nebraska          2   \n",
      "\n",
      "     Games Played   MIN  FGM  FGA    FG%  15:00  3PA    3P%  FTM  FTA    FT%  \\\n",
      "0               8   173   30   85   35.3     12   32   37.5   21   26   80.8   \n",
      "1              30   947   90  177   50.8      5   18   27.8   32   41   78.0   \n",
      "2              26   617   82  218   37.6     19   64   29.7   35   42   83.3   \n",
      "3              31   721   75  195   38.5     21   68   30.9   17   21   81.0   \n",
      "4              24   137   16   50   32.0      7   20   35.0   11   12   91.7   \n",
      "5              14    90    9   34   26.5      2    9   22.2    6    6  100.0   \n",
      "6              16   112    9   34   26.5      4   15   26.7    2    2  100.0   \n",
      "7              26   847  166  319   52.0     70  150   46.7   40   46   87.0   \n",
      "8              30   834  131  346   37.9     29  103   28.2  104  129   80.6   \n",
      "9               7   103   14   38   36.8      2   11   18.2    6    6  100.0   \n",
      "10             30   843   93  183   50.8     20   62   32.3   38   51   74.5   \n",
      "11             28   833  154  303   50.8      0    3    0.0   91  158   57.6   \n",
      "12             25   133   20   53   37.7      2    8   25.0    9   12   75.0   \n",
      "13             22   146   18   44   40.9      0   10    0.0    5    8   62.5   \n",
      "14              4    22    4    4  100.0      0    0    0.0    0    0    0.0   \n",
      "15             15   237   25   44   56.8      1    1  100.0    7    7  100.0   \n",
      "16             20   128   10   31   32.3      0    0    0.0   11   18   61.1   \n",
      "17              5    59    8   14   57.1      0    0    0.0    3    4   75.0   \n",
      "18             12    50    2   12   16.7      0    0    0.0    3    4   75.0   \n",
      "19             29   952  201  417   48.2     46  123   37.4  136  171   79.5   \n",
      "20             29   598   80  192   41.7     32   93   34.4   25   33   75.8   \n",
      "21             28   655   85  231   36.8      9   50   18.0   56   84   66.7   \n",
      "22             25   657   81  205   39.5     18   57   31.6   58   71   81.7   \n",
      "23             19   112   14   26   53.8      0    0    0.0   16   19   84.2   \n",
      "24              2    32    9   15   60.0      0    1    0.0    8   11   72.7   \n",
      "25             22   682  167  293   57.0      0    0    0.0  127  154   82.5   \n",
      "26             30   734  146  362   40.3     29   87   33.3   76  102   74.5   \n",
      "27             30   759   93  219   42.5      9   52   17.3   33   52   63.5   \n",
      "28             29   889  183  383   47.8     40  114   35.1   88  115   76.5   \n",
      "29             29   911  189  370   51.1      0    2    0.0   57   65   87.7   \n",
      "30             24   676   94  258   36.4      8   32   25.0   54   67   80.6   \n",
      "31             26   218   19   39   48.7      0    0    0.0   16   20   80.0   \n",
      "32             28   365   40  105   38.1     13   45   28.9    7   12   58.3   \n",
      "33             30   996  165  326   50.6     48  100   48.0   78   94   83.0   \n",
      "34             23   286   32   69   46.4      0    3    0.0   23   36   63.9   \n",
      "35              7    52    8   14   57.1      1    1  100.0    0    0    0.0   \n",
      "36             16   217   32   57   56.1      0    0    0.0    6   12   50.0   \n",
      "37             22   673  104  199   52.3     23   60   38.3   24   29   82.8   \n",
      "38             29   755  168  338   49.7      8   30   26.7   31   36   86.1   \n",
      "39             30   848  160  240   66.7      1    2   50.0   49   68   72.1   \n",
      "40             30   569   98  243   40.3     25   91   27.5   33   43   76.7   \n",
      "41             18    81   16   43   37.2     12   30   40.0    5    5  100.0   \n",
      "42             28   680   79  178   44.4      0    5    0.0   51   61   83.6   \n",
      "43             31   650   96  207   46.4      3    8   37.5   58   95   61.1   \n",
      "44             28   796  154  380   40.5     88  225   39.1  118  130   90.8   \n",
      "45             20   591  121  255   47.5     22   66   33.3  112  118   94.9   \n",
      "46             30   939  133  272   48.9      0    1    0.0   51   78   65.4   \n",
      "47             30   377   48   96   50.0      0    1    0.0   32   55   58.2   \n",
      "48             18   508  105  220   47.7     11   33   33.3   31   34   91.2   \n",
      "49             23   617   89  233   38.2     25   79   31.6   56   65   86.2   \n",
      "50             26   282   36   86   41.9      1    3   33.3   15   22   68.2   \n",
      "51             30   767  130  321   40.5     42  129   32.6   34   40   85.0   \n",
      "52             30   579   65  112   58.0      0    0    0.0   29   32   90.6   \n",
      "53             20   386   36   92   39.1      9   35   25.7   21   24   87.5   \n",
      "54             15    61    4   16   25.0      0    0    0.0    5    6   83.3   \n",
      "55             30   926  165  365   45.2     20   60   33.3   92  117   78.6   \n",
      "56              4    42    3    9   33.3      3    6   50.0    0    0    0.0   \n",
      "57             29   410   56  119   47.1      1    3   33.3   14   20   70.0   \n",
      "58             31   832  154  300   51.3      1    2   50.0   55   85   64.7   \n",
      "59             29   499   79  218   36.2     40  114   35.1   47   55   85.5   \n",
      "60             28   481   89  184   48.4      4   13   30.8   18   22   81.8   \n",
      "61             27   762  151  341   44.3     50  116   43.1   39   55   70.9   \n",
      "62             24   371   50  140   35.7     12   49   24.5   30   39   76.9   \n",
      "63             25   278   20   52   38.5     13   29   44.8   17   20   85.0   \n",
      "64             10    50    2   12   16.7      0    3    0.0    8   10   80.0   \n",
      "65             10    78    9   16   56.3      0    0    0.0    4    5   80.0   \n",
      "66             29   715  116  245   47.3      8   21   38.1   28   37   75.7   \n",
      "67             30   932  178  420   42.4     47  123   38.2  114  134   85.1   \n",
      "68             29   463   47  124   37.9     11   32   34.4   11   15   73.3   \n",
      "69             29   833  164  299   54.8     22   49   44.9  117  142   82.4   \n",
      "70             23   208   27   75   36.0     20   55   36.4    3    4   75.0   \n",
      "71             29   475   62  163   38.0     12   32   37.5   49   65   75.4   \n",
      "72             29   369   60  140   42.9      5   23   21.7   36   45   80.0   \n",
      "73             14   142   23   43   53.5      9   21   42.9   10   10  100.0   \n",
      "74             30   889   91  239   38.1     25   83   30.1  111  129   86.0   \n",
      "75             31   433   78  141   55.3      0    0    0.0   15   16   93.8   \n",
      "76             27   882  128  337   38.0     47  147   32.0  108  118   91.5   \n",
      "77             21   224   11   30   36.7      0    1    0.0   10   14   71.4   \n",
      "78             30   504   64  157   40.8     14   52   26.9   65   81   80.2   \n",
      "79             28   610   73  210   34.8     29   78   37.2   50   58   86.2   \n",
      "80             23   455   62  116   53.4      0    0    0.0   10   19   52.6   \n",
      "81             29   576   50   98   51.0      0    1    0.0   41   52   78.8   \n",
      "82             29   845  119  284   41.9     67  194   34.5   44   49   89.8   \n",
      "83             29   737   81  149   54.4      0    0    0.0   37   61   60.7   \n",
      "84              7    28    3    7   42.9      0    0    0.0    0    0    0.0   \n",
      "85             30   900  124  320   38.8      8   53   15.1   73   81   90.1   \n",
      "86             30   623   70  182   38.5     31   92   33.7   62   75   82.7   \n",
      "87             23   314   21   50   42.0      0   11    0.0    6    9   66.7   \n",
      "88             22   520   69  153   45.1     12   34   35.3   27   36   75.0   \n",
      "89             20   238   48  100   48.0      0    1    0.0   23   30   76.7   \n",
      "90              9    16    1    3   33.3      0    0    0.0    1    2   50.0   \n",
      "91             14    52    2   14   14.3      0    5    0.0    2    5   40.0   \n",
      "92             30   539   50  152   32.9     27   79   34.2   27   33   81.8   \n",
      "93             27   218   22   69   31.9      0    3    0.0   29   35   82.9   \n",
      "94             29   904  170  398   42.7     52  132   39.4   98  114   86.0   \n",
      "95             32   717  121  284   42.6     37   93   39.8   85  103   82.5   \n",
      "96             17   294   35  101   34.7      8   28   28.6   13   16   81.3   \n",
      "97             21   514   81  155   52.3      9   20   45.0   20   27   74.1   \n",
      "98             30   529   82  151   54.3      0    0    0.0   43   55   78.2   \n",
      "99             24   448   37  118   31.4     12   51   23.5   20   27   74.1   \n",
      "100            29   315   48  104   46.2      3   13   23.1   17   23   73.9   \n",
      "101            27   243   33   63   52.4      0    4    0.0   30   49   61.2   \n",
      "102            25   203   16   59   27.1      0    4    0.0   16   22   72.7   \n",
      "103            30   948  215  386   55.7     18   49   36.7  129  148   87.2   \n",
      "104            29   459   24   58   41.4     14   35   40.0   17   18   94.4   \n",
      "105            27   626   86  198   43.4     11   49   22.4   47   55   85.5   \n",
      "106            29   402   54  142   38.0     17   51   33.3   15   20   75.0   \n",
      "107            26   238   32   87   36.8     16   48   33.3   16   20   80.0   \n",
      "108            29   378   42   92   45.7      0    3    0.0   22   27   81.5   \n",
      "109            28   254   31   86   36.0     14   40   35.0    2    6   33.3   \n",
      "110            26   719   97  218   44.5     22   60   36.7   62   83   74.7   \n",
      "111            29   614   71  181   39.2     30   89   33.7   44   51   86.3   \n",
      "112            23   408   45  140   32.1     20   74   27.0   38   44   86.4   \n",
      "113            29   354   46  120   38.3     33   94   35.1   14   17   82.4   \n",
      "114            25   703   71  163   43.6      1    7   14.3   13   19   68.4   \n",
      "115             4    12    1    3   33.3      0    0    0.0    0    0    0.0   \n",
      "116            29   348   27   74   36.5      8   35   22.9   25   29   86.2   \n",
      "117            27   756  125  251   49.8     18   41   43.9   30   35   85.7   \n",
      "118            24   280   31   89   34.8     13   46   28.3    6   11   54.5   \n",
      "119            22   260   29   78   37.2      9   26   34.6   29   32   90.6   \n",
      "120            29   865  107  249   43.0     14   41   34.1  118  144   81.9   \n",
      "121            23   242   23   62   37.1     12   35   34.3    8   12   66.7   \n",
      "122            29   795   80  202   39.6     59  149   39.6   26   31   83.9   \n",
      "123            14   348   55  127   43.3     10   30   33.3   38   40   95.0   \n",
      "124            30  1018  167  394   42.4     43  119   36.1  168  186   90.3   \n",
      "125            28   823  162  293   55.3     24   60   40.0   50   58   86.2   \n",
      "126            30   555   47  114   41.2     15   38   39.5   29   44   65.9   \n",
      "127            27   806  103  244   42.2     50  134   37.3   17   24   70.8   \n",
      "128            28   745  108  310   34.8     59  163   36.2   42   52   80.8   \n",
      "129            25   296   25   78   32.1      2   10   20.0   20   30   66.7   \n",
      "130            25   189   19   50   38.0     13   32   40.6    4    8   50.0   \n",
      "131            29   895  222  336   66.1      0    0    0.0  128  162   79.0   \n",
      "132            31   820  105  297   35.4     23   70   32.9   44   65   67.7   \n",
      "133            18   462   69  191   36.1     27   89   30.3   75   80   93.8   \n",
      "134            17    74    6   14   42.9      0    0    0.0    5    6   83.3   \n",
      "135            30   604   80  213   37.6     35  101   34.7   22   24   91.7   \n",
      "136            29   483   79  165   47.9     11   41   26.8   41   43   95.3   \n",
      "137            29   703   77  217   35.5      0    4    0.0   71   96   74.0   \n",
      "138            29   861  144  331   43.5     43  112   38.4  136  161   84.5   \n",
      "139            22   127   12   25   48.0      0    1    0.0    4    6   66.7   \n",
      "140            27   671   83  238   34.9     17   69   24.6   94  102   92.2   \n",
      "141            29   952  227  509   44.6     18   56   32.1  110  135   81.5   \n",
      "142            30   356   59  140   42.1     11   47   23.4   22   28   78.6   \n",
      "\n",
      "     OREB  DREB  REB  AST  STL  BLK  TO  PTS  DD2  TD3  \n",
      "0       6    22   28   12    3    6  12   93    0    0  \n",
      "1      19    82  101   72   63   13  40  217    0    0  \n",
      "2       4    36   40   78   22    3  24  218    0    0  \n",
      "3      35   134  169   65   20   10  38  188    2    0  \n",
      "4       3     9   12   12    7    0  14   50    0    0  \n",
      "5       3    13   16   11    5    0  11   26    0    0  \n",
      "6       1    14   15    5    4    3   3   24    0    0  \n",
      "7       9    83   92   95   20   13  59  442    0    0  \n",
      "8      52    75  127   40   47   19  37  395    0    0  \n",
      "9       3     7   10   10    5    0   2   36    0    0  \n",
      "10     29    97  126   50   22    4  32  244    0    0  \n",
      "11     34   158  192  136   48   11  87  399    4    0  \n",
      "12      5    18   23    7    4    5  12   51    0    0  \n",
      "13     12    28   40    5    3    9   6   41    0    0  \n",
      "14      0     4    4    1    2    0   3    8    0    0  \n",
      "15     16    42   58    8    1   11  16   58    0    0  \n",
      "16     16    21   37    9    5    2  10   31    0    0  \n",
      "17      4    13   17    3    0    1   3   19    0    0  \n",
      "18      2     7    9    2    0    0   7    7    0    0  \n",
      "19     43   206  249   78   29   47  68  584    8    0  \n",
      "20      7    50   57   58   15    5  44  217    0    0  \n",
      "21     29    56   85   52   23    7  31  235    0    0  \n",
      "22     12    25   37   98   23    4  53  238    0    0  \n",
      "23     11    14   25    2    7    1   7   44    0    0  \n",
      "24      3     5    8    5    3    0   2   26    0    0  \n",
      "25     43   129  172   39   13   54  52  461    6    0  \n",
      "26     25    94  119   59   18   17  49  397    1    0  \n",
      "27     42    71  113   42   28   13  50  228    0    0  \n",
      "28     37   205  242  127   43   53  80  494   10    1  \n",
      "29     31   124  155   47   28   12  42  435    2    0  \n",
      "30     10    59   69  104   17    5  56  250    2    0  \n",
      "31     10    29   39    9    5    4  22   54    0    0  \n",
      "32     10    71   81   15    9   11  13  100    1    0  \n",
      "33     19    80   99  132   29    7  61  456    1    0  \n",
      "34     31    47   78   13    8   15  21   87    0    0  \n",
      "35      3     7   10    7    1    1   5   17    0    0  \n",
      "36     28    34   62    5    6    8  18   70    0    0  \n",
      "37     13    75   88  175   22    5  64  255   10    0  \n",
      "38     38    84  122   60   15    6  39  375    1    0  \n",
      "39     35   140  175   46   16   11  50  370    2    0  \n",
      "40     29    84  113   19   17   18  26  254    0    0  \n",
      "41      6     4   10    4    4    4   7   49    0    0  \n",
      "42     13    73   86  106   33    4  58  209    0    0  \n",
      "43     48    91  139   32   29    8  43  253    1    0  \n",
      "44      8    69   77   76   16    9  56  514    0    0  \n",
      "45     31    98  129   32   20   31  28  376    3    0  \n",
      "46     99   116  215   43   32   64  36  317    4    0  \n",
      "47     35    61   96    5    5    4  21  128    0    0  \n",
      "48     33    72  105   52   21   27  30  252    1    0  \n",
      "49     23    58   81   70   34    5  30  259    0    0  \n",
      "50     17    44   61    5    4    8  17   88    0    0  \n",
      "51     11    57   68  117   38    1  68  336    0    0  \n",
      "52     58    74  132   35   18    7  37  159    0    0  \n",
      "53      9    26   35   24   11    8  13  102    0    0  \n",
      "54      7     2    9    0    1    3   5   13    0    0  \n",
      "55     73   199  272   50   37   13  67  442   13    0  \n",
      "56      0     3    3    1    0    0   4    9    0    0  \n",
      "57     43    75  118   14    9   23  22  127    1    0  \n",
      "58     66   134  200   46   26   24  63  364    5    0  \n",
      "59      7    20   27   49   12    1  22  245    0    0  \n",
      "60     31    56   87   28    8    5  35  200    0    0  \n",
      "61      9    55   64  118   45    4  58  391    1    0  \n",
      "62     15    34   49   17   13   19  32  142    0    0  \n",
      "63      3    19   22   13    5    0  15   70    0    0  \n",
      "64      5     6   11    6    2    2   3   12    0    0  \n",
      "65      5    13   18    2    1    9   3   22    0    0  \n",
      "66     50   139  189   46   18   50  57  268    4    0  \n",
      "67     24    72   96  103   41   11  83  517    0    0  \n",
      "68     11    46   57   39   30    1  24  116    0    0  \n",
      "69    108   226  334   40   29   46  46  467   17    0  \n",
      "70      2    20   22    5    7    1   6   77    0    0  \n",
      "71     10    33   43   32   13    3  48  185    0    0  \n",
      "72     11    43   54   11    9    2  22  161    0    0  \n",
      "73      4    10   14    6    1    1  13   65    0    0  \n",
      "74     45    75  120   65   39    5  50  318    0    0  \n",
      "75     40    47   87   17   13   15  30  171    0    0  \n",
      "76     12    93  105   59   32    5  54  411    0    0  \n",
      "77     19    26   45   13    6    2   9   32    0    0  \n",
      "78     36    59   95   24   20    7  21  207    0    0  \n",
      "79     11    42   53   91   13    4  72  225    0    0  \n",
      "80     39    71  110   16    8    9  21  134    1    0  \n",
      "81     63   122  185   21    8   32  33  141    3    0  \n",
      "82      9    50   59   91   20    8  48  349    0    0  \n",
      "83     97   172  269   30   15   31  45  199    2    0  \n",
      "84      0     5    5    0    1    4   2    6    0    0  \n",
      "85     27    88  115  206   29    1  82  329    3    0  \n",
      "86     12    57   69  108   26    9  50  233    0    0  \n",
      "87      8    28   36   47   13    1  18   48    0    0  \n",
      "88      8    46   54   90   11    2  44  177    0    0  \n",
      "89     22    35   57    6   11    7  10  119    0    0  \n",
      "90      3     4    7    1    1    0   3    3    0    0  \n",
      "91      2     0    2    4    1    0   4    6    0    0  \n",
      "92      7    53   60   25    8    4  34  154    0    0  \n",
      "93      8    26   34   27   10    0  26   73    0    0  \n",
      "94     50   106  156   99   53   13  56  490    3    0  \n",
      "95     19   103  122   67   22   11  48  364    0    0  \n",
      "96      9    34   43   19    7    0  15   91    1    0  \n",
      "97      6    31   37   92   33    2  43  191    0    0  \n",
      "98     31    70  101   21   11   16  25  207    0    0  \n",
      "99      7    52   59   69   17    3  23  106    0    0  \n",
      "100    25    38   63   16   11   19  20  116    0    0  \n",
      "101    24    22   46    8    2    1  13   96    0    0  \n",
      "102    16    30   46    6    5    6  14   48    0    0  \n",
      "103    57   179  236   63   53   14  47  577    9    0  \n",
      "104     1    48   49   78   12    5  27   79    0    0  \n",
      "105    10    34   44   87   38    5  39  230    1    0  \n",
      "106    13    49   62   48   12    4  33  140    0    0  \n",
      "107     2    27   29   20    4    0  12   96    0    0  \n",
      "108    19    29   48   16    8    8  25  106    0    0  \n",
      "109    13    51   64   15    9   12  17   78    0    0  \n",
      "110    46   135  181   40   31    9  42  278    2    0  \n",
      "111    12    34   46   96   24    1  43  216    0    0  \n",
      "112     6    26   32   16   19    3  26  148    0    0  \n",
      "113    12    40   52   24   22    0  24  139    0    0  \n",
      "114    42   138  180   41   40   17  34  156    0    0  \n",
      "115     0     2    2    0    0    0   2    2    0    0  \n",
      "116     9    19   28   33   21    3  23   87    0    0  \n",
      "117    12    70   82  108   17    1  39  298    1    0  \n",
      "118    12    12   24   23   13    5  11   81    0    0  \n",
      "119     4    13   17   10   11    1  12   96    0    0  \n",
      "120    30    92  122   87   23    8  62  346    1    0  \n",
      "121    12    26   38   17   10    1  12   66    0    0  \n",
      "122    15    71   86   30   36    2  23  245    0    0  \n",
      "123    13    35   48   35   21    4  18  158    0    0  \n",
      "124    21    86  107  173   38   24  83  545    1    0  \n",
      "125    35   121  156   65   14   37  65  398    3    0  \n",
      "126    28    58   86   50   22    8  28  138    0    0  \n",
      "127     7    46   53  177   31    3  57  273    1    0  \n",
      "128    21    85  106   68   28   17  43  317    0    0  \n",
      "129     3    11   14   51   13    2  25   72    0    0  \n",
      "130     3    18   21    6    4    3   2   55    0    0  \n",
      "131   113   184  297   39   39   61  71  572   16    0  \n",
      "132    23    87  110   66   36   14  61  277    0    0  \n",
      "133     5    29   34   47   16    1  26  240    0    0  \n",
      "134     3    13   16    1    3    3   8   17    0    0  \n",
      "135    38    89  127   24   23   22  24  217    1    0  \n",
      "136    42    82  124    9   15    7  23  210    0    0  \n",
      "137    45   120  165   68   30   16  47  225    2    0  \n",
      "138    28    89  117   69   37    8  50  467    0    0  \n",
      "139     5    18   23    3    1    3   8   28    0    0  \n",
      "140    16    70   86   39   31    5  40  277    0    0  \n",
      "141    56   212  268   75   21   22  71  582   11    0  \n",
      "142    11    13   24   30   18    1  32  151    0    0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 50\n",
    "wnba = pd.read_csv(\"wnba.csv\")\n",
    "shape = wnba.shape\n",
    "\n",
    "print('Shape of data:', shape)\n",
    "print('Dataset:', wnba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Frequency Distribution Tables\n",
    "\n",
    "A frequency distribution table has two columns. One column records the unique values of a variable, and the other the frequency of each unique value.\n",
    "![table](https://s3.amazonaws.com/dq-content/285/s1m3_freq_table_anatomy.svg)\n",
    "\n",
    "To generate a frequency distribution table using Python, we can use the `Series.value_counts()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G      60\n",
      "F      33\n",
      "C      25\n",
      "G/F    13\n",
      "F/C    12\n",
      "Name: Pos, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Pos'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Series.value_counts() method, generate frequency distribution tables for the following columns:\n",
    "  * Pos. Assign the frequency distribution table to a variable named freq_distro_pos.\n",
    "  * Height. Assign the frequency distribution table to a variable named freq_distro_height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: \n",
      " G      60\n",
      "F      33\n",
      "C      25\n",
      "G/F    13\n",
      "F/C    12\n",
      "Name: Pos, dtype: int64\n",
      "\n",
      "\n",
      "Height: \n",
      " 188    20\n",
      "193    18\n",
      "175    16\n",
      "185    15\n",
      "183    11\n",
      "173    11\n",
      "191    11\n",
      "196     9\n",
      "178     8\n",
      "180     7\n",
      "170     6\n",
      "198     5\n",
      "201     2\n",
      "168     2\n",
      "206     1\n",
      "165     1\n",
      "Name: Height, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_distro_pos = wnba['Pos'].value_counts()\n",
    "freq_distro_height = wnba['Height'].value_counts()\n",
    "\n",
    "print('Position:','\\n',freq_distro_pos)\n",
    "print('\\n')\n",
    "print('Height:','\\n',freq_distro_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Sorting Frequency Distribution Tables\n",
    "\n",
    "![Sort](https://s3.amazonaws.com/dq-content/284/s1m2_interval_ratio.svg)\n",
    "\n",
    "Because the Height variable has direction, we might be interested to find:\n",
    "* How many players are under 170 cm?\n",
    "* How many players are very tall (over 185)?\n",
    "* Are there any players below 160 cm?\n",
    "\n",
    "It's time-consuming to answer these questions using the table above. The solution is to sort the table ourselves.\n",
    "\n",
    "`wnba['Height'].value_counts()` returns a `Series` object with the measures of height as indices. This allows us to sort the table by index using the `Series.sort_index()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165     1\n",
      "168     2\n",
      "170     6\n",
      "173    11\n",
      "175    16\n",
      "178     8\n",
      "180     7\n",
      "183    11\n",
      "185    15\n",
      "188    20\n",
      "191    11\n",
      "193    18\n",
      "196     9\n",
      "198     5\n",
      "201     2\n",
      "206     1\n",
      "Name: Height, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Height'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also sort the table by index in a descending order using `wnba['Height'].value_counts().sort_index(ascending = False)`\n",
    "\n",
    "Generate a frequency distribution table for the Age variable, which is measured on a ratio scale, and sort the table by unique values.\n",
    "* Sort the table by unique values in an ascending order, and assign the result to a variable named age_ascending.\n",
    "* Sort the table by unique values in a descending order, and assign the result to a variable named age_descending.\n",
    "\n",
    "Using the variable inspector, analyze one of the frequency distribution tables and brainstorm questions that might be interesting to answer here. These include:\n",
    "* How many players are under 20?\n",
    "* How many players are 30 or over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age Ascending: \n",
      " 21     2\n",
      "22    10\n",
      "23    15\n",
      "24    16\n",
      "25    15\n",
      "26    12\n",
      "27    13\n",
      "28    14\n",
      "29     8\n",
      "30     9\n",
      "31     8\n",
      "32     8\n",
      "33     3\n",
      "34     5\n",
      "35     4\n",
      "36     1\n",
      "Name: Age, dtype: int64\n",
      "\n",
      "\n",
      "Age Descending \n",
      " 36     1\n",
      "35     4\n",
      "34     5\n",
      "33     3\n",
      "32     8\n",
      "31     8\n",
      "30     9\n",
      "29     8\n",
      "28    14\n",
      "27    13\n",
      "26    12\n",
      "25    15\n",
      "24    16\n",
      "23    15\n",
      "22    10\n",
      "21     2\n",
      "Name: Age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "age_ascending = wnba['Age'].value_counts().sort_index()\n",
    "age_descending = wnba['Age'].value_counts().sort_index(ascending = False)\n",
    "\n",
    "print('Age Ascending:','\\n', age_ascending)\n",
    "print('\\n')\n",
    "print('Age Descending','\\n', age_descending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Sorting Tables for Ordinal Variables\n",
    "\n",
    "We name the new column PTS_ordinal_scale. Below is a short extract from our data set containing the new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name  PTS         PTS_ordinal_scale\n",
      "0    Aerial Powers   93   many, but below average\n",
      "1      Alana Beard  217  average number of points\n",
      "2     Alex Bentley  218  average number of points\n",
      "3  Alex Montgomery  188  average number of points\n",
      "4     Alexis Jones   50                few points\n"
     ]
    }
   ],
   "source": [
    "def make_pts_ordinal(row):\n",
    "    if row['PTS'] <= 20:\n",
    "        return 'very few points'\n",
    "    if (20 < row['PTS'] <=  80):\n",
    "        return 'few points'\n",
    "    if (80 < row['PTS'] <=  150):\n",
    "        return 'many, but below average'\n",
    "    if (150 < row['PTS'] <= 300):\n",
    "        return 'average number of points'\n",
    "    if (300 < row['PTS'] <=  450):\n",
    "        return 'more than average'\n",
    "    else:\n",
    "        return 'much more than average'\n",
    "    \n",
    "wnba['PTS_ordinal_scale'] = wnba.apply(make_pts_ordinal, axis = 1)\n",
    "\n",
    "print(wnba[['Name', 'PTS', 'PTS_ordinal_scale']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the frequency distribution table for the `PTS_ordinal_scale` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of points    45\n",
      "few points                  27\n",
      "many, but below average     25\n",
      "more than average           21\n",
      "much more than average      13\n",
      "very few points             12\n",
      "Name: PTS_ordinal_scale, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS_ordinal_scale'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sort the labels in an ascending or descending order, but using `Series.sort_index()` doesn't work because the method can't infer quantities from words like \"few points\". `Series.sort_index()` can only order the index alphabetically in an ascending or descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of points    45\n",
      "few points                  27\n",
      "many, but below average     25\n",
      "more than average           21\n",
      "much more than average      13\n",
      "very few points             12\n",
      "Name: PTS_ordinal_scale, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS_ordinal_scale'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to do selection by index label. The output of `wnba['PTS_ordinal_scale'].value_counts()` is a Series object with the labels as indices. This means we can select by indices to reorder in any way we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very few points             12\n",
      "few points                  27\n",
      "many, but below average     25\n",
      "average number of points    45\n",
      "Name: PTS_ordinal_scale, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS_ordinal_scale'].value_counts()[['very few points', 'few points', 'many, but below average', 'average number of points']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be time-consuming because it involves more typing than is ideal. We can use iloc[] instead to reorder by position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more than average           21\n",
      "few points                  27\n",
      "many, but below average     25\n",
      "average number of points    45\n",
      "Name: PTS_ordinal_scale, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS_ordinal_scale'].value_counts().iloc[[3, 1, 2, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a frequency distribution table for the transformed PTS_ordinal_scale column.\n",
    "* Order the table by unique values in a descending order (not alphabetically).\n",
    "* Assign the result to a variable named pts_ordinal_desc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "much more than average      13\n",
      "more than average           21\n",
      "average number of points    45\n",
      "many, but below average     25\n",
      "few points                  27\n",
      "very few points             12\n",
      "Name: PTS_ordinal_scale, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def make_pts_ordinal(row):\n",
    "    if row['PTS'] <= 20:\n",
    "        return 'very few points'\n",
    "    if (20 < row['PTS'] <=  80):\n",
    "        return 'few points'\n",
    "    if (80 < row['PTS'] <=  150):\n",
    "        return 'many, but below average'\n",
    "    if (150 < row['PTS'] <= 300):\n",
    "        return 'average number of points'\n",
    "    if (300 < row['PTS'] <=  450):\n",
    "        return 'more than average'\n",
    "    else:\n",
    "        return 'much more than average'\n",
    "    \n",
    "wnba['PTS_ordinal_scale'] = wnba.apply(make_pts_ordinal, axis = 1)\n",
    "\n",
    "# Type your answer below\n",
    "pts_ordinal_desc = wnba['PTS_ordinal_scale'].value_counts().iloc[[4, 3, 0, 2, 1, 5]]\n",
    "print(pts_ordinal_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Proportions and Percentages\n",
    "\n",
    "In pandas, we can compute all the proportions at once by dividing each frequency by the total number of players:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G      0.419580\n",
      "F      0.230769\n",
      "C      0.174825\n",
      "G/F    0.090909\n",
      "F/C    0.083916\n",
      "Name: Pos, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Pos'].value_counts() / len(wnba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's slightly faster though to use `Series.value_counts()` with the normalize parameter set to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G      0.419580\n",
      "F      0.230769\n",
      "C      0.174825\n",
      "G/F    0.090909\n",
      "F/C    0.083916\n",
      "Name: Pos, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Pos'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find percentages, we just have to multiply the proportions by 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G      41.958042\n",
      "F      23.076923\n",
      "C      17.482517\n",
      "G/F     9.090909\n",
      "F/C     8.391608\n",
      "Name: Pos, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Pos'].value_counts(normalize = True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions about the Age variable:\n",
    "* What proportion of players are 25 years old? Assign your answer to a variable named proportion_25.\n",
    "* What percentage of players are 30 years old? Assign your answer to a variable named percentage_30.\n",
    "* What percentage of players are 30 years or older? Assign your answer to a variable named percentage_over_30.\n",
    "* What percentage of players are 23 years or younger? Assign your answer to a variable named percentage_below_23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24    0.111888\n",
      "23    0.104895\n",
      "25    0.104895\n",
      "28    0.097902\n",
      "27    0.090909\n",
      "26    0.083916\n",
      "22    0.069930\n",
      "30    0.062937\n",
      "31    0.055944\n",
      "29    0.055944\n",
      "32    0.055944\n",
      "34    0.034965\n",
      "35    0.027972\n",
      "33    0.020979\n",
      "21    0.013986\n",
      "36    0.006993\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Age'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24    11.188811\n",
      "23    10.489510\n",
      "25    10.489510\n",
      "28     9.790210\n",
      "27     9.090909\n",
      "26     8.391608\n",
      "22     6.993007\n",
      "30     6.293706\n",
      "31     5.594406\n",
      "29     5.594406\n",
      "32     5.594406\n",
      "34     3.496503\n",
      "35     2.797203\n",
      "33     2.097902\n",
      "21     1.398601\n",
      "36     0.699301\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Age'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age_ordinal_scale\n",
      "other ages           54.545455\n",
      "over 30              26.573427\n",
      "under 23             18.881119\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_age_ordinal(row):\n",
    "    if row['Age'] <= 23:\n",
    "        return 'under 23'\n",
    "    if (20 < row['Age'] >=  30):\n",
    "        return 'over 30'\n",
    "    else:\n",
    "        return 'other ages'\n",
    "    \n",
    "wnba['Age_ordinal_scale'] = wnba.apply(make_age_ordinal, axis = 1)\n",
    "\n",
    "print(wnba[['Age_ordinal_scale']].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Percentiles and Percentile Ranks\n",
    "\n",
    "In the previous exercise, we found that the percentage of players aged 23 years or younger is 19% (rounded to the nearest integer). This percentage is also called a **percentile rank**.\n",
    "\n",
    "In this context, the value of 23 is called the 19th **percentile**. If a value *x* is the 19th percentile, it means that 19% of *all* the values in the distribution are equal to or less than *x*.\n",
    "\n",
    "![percent](https://s3.amazonaws.com/dq-content/285/s1m3_percentiles_v2.svg)\n",
    "\n",
    "In our previous exercise, our answer to this question was 18.881%. We can arrive at the same answer a bit faster using the `percentileofscore(a, score, kind='weak')` <span style=\"color:green\">function</span> from `scipy.stats`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.88111888111888\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "print(percentileofscore(a = wnba['Age'], score = 23, kind = 'weak'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use `kind = 'weak'` to indicate that we want to find the percentage of values that are equal to or less than the value we specify in the score parameter.\n",
    "\n",
    "Another question we had was what percentage of players are 30 years or older. We can answer this question too using percentile ranks. First we need to find the percentage of values equal to or less than 29 years (the percentile rank of 29). The rest of the values must be 30 years or more.\n",
    "\n",
    "![age](https://s3.amazonaws.com/dq-content/285/s1m3_difference.svg)\n",
    "\n",
    "In our exercise the answer we found was 26.573%. This is what we get using the technique we've just learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.573426573426573\n"
     ]
    }
   ],
   "source": [
    "print(100 - percentileofscore(wnba['Age'], 29, kind = 'weak'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `percentileofscore()` from `scipy.stats` has been imported above. Use it to answer the following: \n",
    "\n",
    "* What percentage of players played half the number of games or less in the 2016-2017 season (there are 34 games in the WNBAâ€™s regular season)? Use the `Games Played` column to find the data you need, and assign your answer to a variable named `percentile_rank_half_less`.\n",
    "* What percentage of players played more than half the number of games of the season 2016-2017? Assign your result to `percentage_half_more`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.083916083916083\n",
      "83.91608391608392\n"
     ]
    }
   ],
   "source": [
    "percentile_rank_half_less = percentileofscore(wnba['Games Played'], 17, kind='weak')\n",
    "percentage_half_more = 100 - percentile_rank_half_less\n",
    "\n",
    "print(percentile_rank_half_less)\n",
    "print(percentage_half_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Finding Percentiles with pandas\n",
    "\n",
    "To find percentiles, we can use the `Series.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    143.000000\n",
      "mean      27.076923\n",
      "std        3.679170\n",
      "min       21.000000\n",
      "25%       24.000000\n",
      "50%       27.000000\n",
      "75%       30.000000\n",
      "max       36.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can use `iloc[]` to isolate just the output we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min    21.0\n",
      "25%    24.0\n",
      "50%    27.0\n",
      "75%    30.0\n",
      "max    36.0\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Age'].describe().iloc[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AgePercent](https://s3.amazonaws.com/dq-content/285/s1m3_quantiles_v2.svg)\n",
    "\n",
    "The three percentiles that divide the distribution in four equal parts are also known as `quartiles`.\n",
    "* The first quartile (also called lower quartile) is 24 (note that 24 is also the 25th percentile).\n",
    "* The second quartile (also called the middle quartile) is 27 (note that 27 is also the 50th percentile).\n",
    "* And the third quartile (also called the upper quartile) is 30 (note that 30 is also the 75th percentile).\n",
    "\n",
    "We may be interested to find the percentiles for percentages other than 25%, 50%, or 75%. For that, we can use the `percentiles` parameter of `Series.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min      21.0\n",
      "10%      23.0\n",
      "15%      23.0\n",
      "33%      25.0\n",
      "50%      27.0\n",
      "59.2%    28.0\n",
      "85%      31.0\n",
      "90%      32.0\n",
      "max      36.0\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Age'].describe(percentiles = [.1, .15, .33, .5, .592, .85, .9]).iloc[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `Age` variable along with `Series.describe()` to answer the following questions:\n",
    "* What's the upper quartile of the `Age` variable? Assign your answer to a variable named `age_upper_quartile`.\n",
    "* What's the middle quartile of the `Age` variable? Assign your answer to a variable named `age_middle_quartile`.\n",
    "* What's the 95th percentile of the `Age` variable? Assign your answer to a variable named `age_95th_percentile`.\n",
    "\n",
    "Indicate the truth value of the following sentences:\n",
    "* A percentile is a value of a variable, and it corresponds to a certain percentile rank in the distribution of that variable. (If you think this is true, assign `True` (boolean, not string) to a variable named `question1`, otherwise assign `False`.)\n",
    "* A percentile rank is a numerical value from the distribution of a variable. (Assign `True` or `False` to `question2`.)\n",
    "The 25th percentile is the same thing as the lower quartile, and the upper quartile is the same thing as the third quartile. (Assign `True` or `False` to `question3`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n",
      "27.0\n",
      "34.0\n"
     ]
    }
   ],
   "source": [
    "wnba = pd.read_csv('wnba.csv')\n",
    "percentiles = wnba['Age'].describe(percentiles = [.5, .75, .95])\n",
    "age_upper_quartile = percentiles['75%']\n",
    "age_middle_quartile = percentiles['50%']\n",
    "age_95th_percentile = percentiles['95%']\n",
    "\n",
    "question1 = True\n",
    "question2 = False\n",
    "question3 = True\n",
    "\n",
    "print(age_upper_quartile)\n",
    "print(age_middle_quartile)\n",
    "print(age_95th_percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Grouped Frequency Distribution Tables\n",
    "\n",
    "Not all frequency tables are straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.0      1\n",
      "57.0      1\n",
      "58.0      1\n",
      "59.0      2\n",
      "62.0      1\n",
      "63.0      3\n",
      "64.0      5\n",
      "65.0      4\n",
      "66.0      8\n",
      "67.0      1\n",
      "68.0      2\n",
      "69.0      2\n",
      "70.0      3\n",
      "71.0      2\n",
      "73.0      6\n",
      "74.0      4\n",
      "75.0      4\n",
      "76.0      4\n",
      "77.0     10\n",
      "78.0      5\n",
      "79.0      6\n",
      "80.0      3\n",
      "81.0      5\n",
      "82.0      4\n",
      "83.0      4\n",
      "84.0      9\n",
      "85.0      2\n",
      "86.0      7\n",
      "87.0      6\n",
      "88.0      6\n",
      "89.0      3\n",
      "90.0      2\n",
      "91.0      3\n",
      "93.0      3\n",
      "95.0      2\n",
      "96.0      2\n",
      "97.0      1\n",
      "104.0     2\n",
      "108.0     1\n",
      "113.0     2\n",
      "Name: Weight, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Weight'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, pandas can handle this process gracefully. We only need to make use of the `bins` parameter of `Series.value_counts()`. We want ten equal intervals, so we need to specify `bins = 10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54.941, 60.8]     5\n",
      "(60.8, 66.6]      21\n",
      "(66.6, 72.4]      10\n",
      "(72.4, 78.2]      33\n",
      "(78.2, 84.0]      31\n",
      "(84.0, 89.8]      24\n",
      "(89.8, 95.6]      10\n",
      "(95.6, 101.4]      3\n",
      "(101.4, 107.2]     2\n",
      "(107.2, 113.0]     3\n",
      "Name: Weight, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['Weight'].value_counts(bins = 10).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(54.941, 60.8], (60.8, 66.6] or (107.2, 113.0]` are number intervals. The `(` character indicates that the starting point is not included, while the `]` indicates that the endpoint is included. The interval `(54.941, 60.8]` contains all real numbers greater than 54.941 and less than or equal to 60.8.\n",
    "\n",
    "Examine the frequency table for the PTS (total points) variable trying to find some patterns in the distribution of values. Then, generate a grouped frequency distribution table for the PTS variable with the following characteristics:\n",
    "* The table has 10 class intervals.\n",
    "* For each class interval, the table shows percentages instead of frequencies.\n",
    "* The class intervals are sorted in descending order. \n",
    "\n",
    "Assign the table to a variable named grouped_freq_table, then print it and try again to find some patterns in the distribution of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525.8, 584.0]     3.496503\n",
      "(467.6, 525.8]     2.797203\n",
      "(409.4, 467.6]     5.594406\n",
      "(351.2, 409.4]     6.993007\n",
      "(293.0, 351.2]     5.594406\n",
      "(234.8, 293.0]    11.888112\n",
      "(176.6, 234.8]    13.986014\n",
      "(118.4, 176.6]    11.888112\n",
      "(60.2, 118.4]     16.783217\n",
      "(1.417, 60.2]     20.979021\n",
      "Name: PTS, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "grouped_freq_table = wnba['PTS'].value_counts(bins = 10, normalize = True).sort_index(ascending = False)*100\n",
    "\n",
    "print(grouped_freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Information Loss\n",
    "\n",
    "When we generate grouped frequency distribution tables, there's an inevitable information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.417, 60.2]     30\n",
      "(60.2, 118.4]     24\n",
      "(176.6, 234.8]    20\n",
      "(118.4, 176.6]    17\n",
      "(234.8, 293.0]    17\n",
      "(351.2, 409.4]    10\n",
      "(293.0, 351.2]     8\n",
      "(409.4, 467.6]     8\n",
      "(525.8, 584.0]     5\n",
      "(467.6, 525.8]     4\n",
      "Name: PTS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS'].value_counts(bins = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the first interval, we can see there are 30 players who scored between 2 and 60 points (2 is the minimum value in our data set, and points in basketball can only be integers).\n",
    "\n",
    "To get back this granular information, we can increase the number of class intervals. However, if we do that, we end up again with a table that's lengthy and very difficult to analyze.\n",
    "\n",
    "On the other side, if we decrease the number of class intervals, we lose even more information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.417, 118.4]    54\n",
      "(118.4, 234.8]    37\n",
      "(234.8, 351.2]    25\n",
      "(351.2, 467.6]    18\n",
      "(467.6, 584.0]     9\n",
      "Name: PTS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS'].value_counts(bins = 5).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a rule of thumb, 10 is a good number of class intervals to choose because it offers a good balance between information and comprehensibility.\n",
    "\n",
    "Generate a grouped frequency distribution for the MIN variable (minutes played during the season), and experiment with the number of class intervals to get a sense for what conclusions you can draw as you vary the number of class intervals. Try to experiment with the following numbers of class intervals:\n",
    "1\n",
    "2\n",
    "3\n",
    "5\n",
    "10\n",
    "15\n",
    "20\n",
    "40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Readability for Grouped Frequency Tables\n",
    "\n",
    "The intervals pandas outputs are confusing at first sight:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.417, 99.0]     48\n",
      "(99.0, 196.0]     27\n",
      "(196.0, 293.0]    33\n",
      "(293.0, 390.0]    13\n",
      "(390.0, 487.0]    13\n",
      "(487.0, 584.0]     9\n",
      "Name: PTS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wnba['PTS'].value_counts(bins = 6).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one way to code the intervals. We start with creating the intervals using the `pd.interval_range()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntervalIndex([(0, 100], (100, 200], (200, 300], (300, 400], (400, 500], (500, 600]], dtype='interval[int64, right]')\n"
     ]
    }
   ],
   "source": [
    "intervals = pd.interval_range(start = 0, end = 600, freq = 100)\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we pass the `intervals` variable to the `bins` parameter, store the result to `gr_freq_table`, and print the result, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 100]      49\n",
      "(100, 200]    28\n",
      "(200, 300]    32\n",
      "(300, 400]    17\n",
      "(400, 500]    10\n",
      "(500, 600]     7\n",
      "Name: PTS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "gr_freq_table = wnba[\"PTS\"].value_counts(bins = intervals).sort_index()\n",
    "print(gr_freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a quick check of our work. There are 143 players in the data set, so the frequencies should add up to 143:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "print(gr_freq_table.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the techniques above, generate a grouped frequency table for the PTS variable. The table should have the following characteristics:\n",
    "* The first class interval starts at 0 (not included).\n",
    "* The last class interval ends at 600 (included).\n",
    "* Each interval has a range of 60 points.\n",
    "* There are 10 class intervals.\n",
    "\n",
    "Assign the table to a variable named gr_freq_table_10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 60]       30\n",
      "(60, 120]     25\n",
      "(120, 180]    17\n",
      "(180, 240]    22\n",
      "(240, 300]    15\n",
      "(300, 360]     7\n",
      "(360, 420]    11\n",
      "(420, 480]     7\n",
      "(480, 540]     4\n",
      "(540, 600]     5\n",
      "Name: PTS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "intervals = pd.interval_range(start = 0, end = 600, freq = 60)\n",
    "gr_freq_table_10 = wnba['PTS'].value_counts(bins = intervals).sort_index()\n",
    "\n",
    "print(gr_freq_table_10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "102858ec4b04dcd74038fe1ba5e2df4247d5b4c1f27427e1bf6cc1238b63214b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
